env {
  spark.streaming.batchDuration = 5
  spark.app.name = "seatunnel"
  spark.ui.port = 13000
  spark.sql.catalogImplementation = "hive"
}

source {
  file {
      path = "hdfs://192.168.50.36:8020/user/hive/warehouse/user_95.db/autotables_10705"
      result_table_name = "autotables_10705"
  }
}

transform {
}

sink {
  console{
  }
//  Hive {
//    source_table_name = "test_druid_to_hive"
//    result_table_name = "user_85`.`test_druid_to_hive"
//    save_mode = "overwrite"
//    sink_columns = "name,age,detail,address"
//  }
}


// 3.0.3 可以    2.4.8 报错
// ERROR: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the referenced columns only include the internal corrupt record column